{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 105955,
     "status": "ok",
     "timestamp": 1648821893540,
     "user": {
      "displayName": "张月恒",
      "userId": "16697410716860166342"
     },
     "user_tz": 240
    },
    "id": "HgTIHpM7DVC-",
    "outputId": "1f6b3077-13e4-4eb4-8904-9b0a6d09fb8b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: timm in /Users/rubyjiang/opt/anaconda3/envs/pytorch/lib/python3.7/site-packages (0.5.4)\n",
      "Requirement already satisfied: torchvision in /Users/rubyjiang/opt/anaconda3/envs/pytorch/lib/python3.7/site-packages (from timm) (0.12.0)\n",
      "Requirement already satisfied: torch>=1.4 in /Users/rubyjiang/opt/anaconda3/envs/pytorch/lib/python3.7/site-packages (from timm) (1.11.0)\n",
      "Requirement already satisfied: typing-extensions in /Users/rubyjiang/opt/anaconda3/envs/pytorch/lib/python3.7/site-packages (from torch>=1.4->timm) (4.1.1)\n",
      "Requirement already satisfied: requests in /Users/rubyjiang/opt/anaconda3/envs/pytorch/lib/python3.7/site-packages (from torchvision->timm) (2.27.1)\n",
      "Requirement already satisfied: numpy in /Users/rubyjiang/opt/anaconda3/envs/pytorch/lib/python3.7/site-packages (from torchvision->timm) (1.19.2)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /Users/rubyjiang/opt/anaconda3/envs/pytorch/lib/python3.7/site-packages (from torchvision->timm) (8.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/rubyjiang/opt/anaconda3/envs/pytorch/lib/python3.7/site-packages (from requests->torchvision->timm) (3.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/rubyjiang/opt/anaconda3/envs/pytorch/lib/python3.7/site-packages (from requests->torchvision->timm) (2021.10.8)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/rubyjiang/opt/anaconda3/envs/pytorch/lib/python3.7/site-packages (from requests->torchvision->timm) (1.26.9)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /Users/rubyjiang/opt/anaconda3/envs/pytorch/lib/python3.7/site-packages (from requests->torchvision->timm) (2.0.12)\n",
      "Requirement already satisfied: torchvision in /Users/rubyjiang/opt/anaconda3/envs/pytorch/lib/python3.7/site-packages (0.12.0)\n",
      "Requirement already satisfied: numpy in /Users/rubyjiang/opt/anaconda3/envs/pytorch/lib/python3.7/site-packages (from torchvision) (1.19.2)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /Users/rubyjiang/opt/anaconda3/envs/pytorch/lib/python3.7/site-packages (from torchvision) (8.4.0)\n",
      "Requirement already satisfied: torch==1.11.0 in /Users/rubyjiang/opt/anaconda3/envs/pytorch/lib/python3.7/site-packages (from torchvision) (1.11.0)\n",
      "Requirement already satisfied: typing-extensions in /Users/rubyjiang/opt/anaconda3/envs/pytorch/lib/python3.7/site-packages (from torchvision) (4.1.1)\n",
      "Requirement already satisfied: requests in /Users/rubyjiang/opt/anaconda3/envs/pytorch/lib/python3.7/site-packages (from torchvision) (2.27.1)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /Users/rubyjiang/opt/anaconda3/envs/pytorch/lib/python3.7/site-packages (from requests->torchvision) (2.0.12)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/rubyjiang/opt/anaconda3/envs/pytorch/lib/python3.7/site-packages (from requests->torchvision) (1.26.9)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/rubyjiang/opt/anaconda3/envs/pytorch/lib/python3.7/site-packages (from requests->torchvision) (3.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/rubyjiang/opt/anaconda3/envs/pytorch/lib/python3.7/site-packages (from requests->torchvision) (2021.10.8)\n"
     ]
    }
   ],
   "source": [
    "!pip install timm\n",
    "!pip install torchvision --upgrade\n",
    "# !mkdir StandfordDog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "Hzc7qq5tDy7Z"
   },
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.models as models\n",
    "from timm.models.layers import trunc_normal_, DropPath\n",
    "from timm.models.registry import register_model\n",
    "import tarfile\n",
    "# from Dataset import *\n",
    "# from prepare import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 21599,
     "status": "ok",
     "timestamp": 1648821954498,
     "user": {
      "displayName": "张月恒",
      "userId": "16697410716860166342"
     },
     "user_tz": 240
    },
    "id": "q3QVcAVfLweD",
    "outputId": "9b7bff14-f7f7-4064-e9ef-3c983e267ee7"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WPvjgUBHFC2x"
   },
   "outputs": [],
   "source": [
    "with tarfile.open('/content/drive/MyDrive/FGVC-project/CUB_200_2011.tgz', 'r:gz') as tar:\n",
    "  tar.extractall(path='/content')\n",
    "\n",
    "#with tarfile.open('/content/drive/MyDrive/FGVC-project/StandfordDog/images.tar', 'r') as tar:\n",
    "  #tar.extractall(path='/content/StandfordDog')\n",
    "\n",
    "#with tarfile.open('/content/drive/MyDrive/FGVC-project/StandfordDog/lists.tar', 'r') as tar:\n",
    "  #tar.extractall(path='/content/StandfordDog')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "W-GJ7T87e2Ca"
   },
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "epochs=50\n",
    "lr = 0.03\n",
    "weight_decay = 1e-3\n",
    "label_smoothing = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1648829834048,
     "user": {
      "displayName": "张月恒",
      "userId": "16697410716860166342"
     },
     "user_tz": 240
    },
    "id": "Iu40C54lHccJ",
    "outputId": "b7c78c24-8676-4e0d-efb1-4eb286a9d144"
   },
   "outputs": [],
   "source": [
    "train_transforms = transforms.Compose([transforms.Resize((256, 256), Image.BILINEAR),\n",
    "                                       transforms.RandomCrop((224, 224)),\n",
    "                                       transforms.RandomHorizontalFlip(),\n",
    "                                       transforms.ToTensor(),\n",
    "                                       transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])])\n",
    "\n",
    "test_transforms = transforms.Compose([transforms.Resize((256, 256), Image.BILINEAR),\n",
    "                                        transforms.CenterCrop((224, 224)),\n",
    "                                        transforms.ToTensor(),\n",
    "                                        transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])])\n",
    "\n",
    "\n",
    "train_dataset = CUB2011(root='/content/', transform=train_transforms, train=True, extract=False)\n",
    "test_dataset = CUB2011(root='/content/', transform=test_transforms, train=False, extract=False)\n",
    "\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, \n",
    "                                              shuffle=True, num_workers=1)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, \n",
    "                                          shuffle=False, num_workers=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ydYcM2d8JSN_"
   },
   "source": [
    "convNext base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "aLFWfT8SNZP2"
   },
   "outputs": [],
   "source": [
    "class ConvNeXt(nn.Module):\n",
    "    r\"\"\" ConvNeXt\n",
    "        A PyTorch impl of : `A ConvNet for the 2020s`  -\n",
    "          https://arxiv.org/pdf/2201.03545.pdf\n",
    "    Args:\n",
    "        in_chans (int): Number of input image channels. Default: 3\n",
    "        num_classes (int): Number of classes for classification head. Default: 1000\n",
    "        depths (tuple(int)): Number of blocks at each stage. Default: [3, 3, 9, 3]\n",
    "        dims (int): Feature dimension at each stage. Default: [96, 192, 384, 768]\n",
    "        drop_path_rate (float): Stochastic depth rate. Default: 0.\n",
    "        layer_scale_init_value (float): Init value for Layer Scale. Default: 1e-6.\n",
    "        head_init_scale (float): Init scaling value for classifier weights and biases. Default: 1.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_chans=3, num_classes=1000, \n",
    "                 depths=[3, 3, 9, 3], dims=[96, 192, 384, 768], drop_path_rate=0., \n",
    "                 layer_scale_init_value=1e-6, head_init_scale=1.,\n",
    "                 ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.downsample_layers = nn.ModuleList() # stem and 3 intermediate downsampling conv layers\n",
    "        stem = nn.Sequential(\n",
    "            nn.Conv2d(in_chans, dims[0], kernel_size=4, stride=4),\n",
    "            LayerNorm(dims[0], eps=1e-6, data_format=\"channels_first\")\n",
    "        )\n",
    "        self.downsample_layers.append(stem)\n",
    "        for i in range(3):\n",
    "            downsample_layer = nn.Sequential(\n",
    "                    LayerNorm(dims[i], eps=1e-6, data_format=\"channels_first\"),\n",
    "                    nn.Conv2d(dims[i], dims[i+1], kernel_size=2, stride=2),\n",
    "            )\n",
    "            self.downsample_layers.append(downsample_layer)\n",
    "\n",
    "        self.stages = nn.ModuleList() # 4 feature resolution stages, each consisting of multiple residual blocks\n",
    "        dp_rates=[x.item() for x in torch.linspace(0, drop_path_rate, sum(depths))] \n",
    "        cur = 0\n",
    "        for i in range(4):\n",
    "            stage = nn.Sequential(\n",
    "                *[Block(dim=dims[i], drop_path=dp_rates[cur + j], \n",
    "                layer_scale_init_value=layer_scale_init_value) for j in range(depths[i])]\n",
    "            )\n",
    "            self.stages.append(stage)\n",
    "            cur += depths[i]\n",
    "\n",
    "        self.norm = nn.LayerNorm(dims[-1], eps=1e-6) # final norm layer\n",
    "        self.head = nn.Linear(dims[-1], num_classes)\n",
    "\n",
    "        self.apply(self._init_weights)\n",
    "        self.head.weight.data.mul_(head_init_scale)\n",
    "        self.head.bias.data.mul_(head_init_scale)\n",
    "\n",
    "    def _init_weights(self, m):\n",
    "        if isinstance(m, (nn.Conv2d, nn.Linear)):\n",
    "            trunc_normal_(m.weight, std=.02)\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    def forward_features(self, x):\n",
    "        for i in range(4):\n",
    "            x = self.downsample_layers[i](x)\n",
    "            x = self.stages[i](x)\n",
    "        return self.norm(x.mean([-2, -1])) # global average pooling, (N, C, H, W) -> (N, C)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.forward_features(x)\n",
    "        x = self.head(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VJ-Grs7ZN1x3"
   },
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    r\"\"\" ConvNeXt Block. There are two equivalent implementations:\n",
    "    (1) DwConv -> LayerNorm (channels_first) -> 1x1 Conv -> GELU -> 1x1 Conv; all in (N, C, H, W)\n",
    "    (2) DwConv -> Permute to (N, H, W, C); LayerNorm (channels_last) -> Linear -> GELU -> Linear; Permute back\n",
    "    We use (2) as we find it slightly faster in PyTorch\n",
    "    \n",
    "    Args:\n",
    "        dim (int): Number of input channels.\n",
    "        drop_path (float): Stochastic depth rate. Default: 0.0\n",
    "        layer_scale_init_value (float): Init value for Layer Scale. Default: 1e-6.\n",
    "    \"\"\"\n",
    "    def __init__(self, dim, drop_path=0., layer_scale_init_value=1e-6):\n",
    "        super().__init__()\n",
    "        self.dwconv = nn.Conv2d(dim, dim, kernel_size=7, padding=3, groups=dim) # depthwise conv\n",
    "        self.norm = LayerNorm(dim, eps=1e-6)\n",
    "        self.pwconv1 = nn.Linear(dim, 4 * dim) # pointwise/1x1 convs, implemented with linear layers\n",
    "        self.act = nn.GELU()\n",
    "        self.pwconv2 = nn.Linear(4 * dim, dim)\n",
    "        self.gamma = nn.Parameter(layer_scale_init_value * torch.ones((dim)), \n",
    "                                    requires_grad=True) if layer_scale_init_value > 0 else None\n",
    "        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n",
    "\n",
    "    def forward(self, x):\n",
    "        input = x\n",
    "        x = self.dwconv(x)\n",
    "        x = x.permute(0, 2, 3, 1) # (N, C, H, W) -> (N, H, W, C)\n",
    "        x = self.norm(x)\n",
    "        x = self.pwconv1(x)\n",
    "        x = self.act(x)\n",
    "        x = self.pwconv2(x)\n",
    "        if self.gamma is not None:\n",
    "            x = self.gamma * x\n",
    "        x = x.permute(0, 3, 1, 2) # (N, H, W, C) -> (N, C, H, W)\n",
    "\n",
    "        x = input + self.drop_path(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "gye1M0NVNwWI"
   },
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    r\"\"\" LayerNorm that supports two data formats: channels_last (default) or channels_first. \n",
    "    The ordering of the dimensions in the inputs. channels_last corresponds to inputs with \n",
    "    shape (batch_size, height, width, channels) while channels_first corresponds to inputs \n",
    "    with shape (batch_size, channels, height, width).\n",
    "    \"\"\"\n",
    "    def __init__(self, normalized_shape, eps=1e-6, data_format=\"channels_last\"):\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(torch.ones(normalized_shape))\n",
    "        self.bias = nn.Parameter(torch.zeros(normalized_shape))\n",
    "        self.eps = eps\n",
    "        self.data_format = data_format\n",
    "        if self.data_format not in [\"channels_last\", \"channels_first\"]:\n",
    "            raise NotImplementedError \n",
    "        self.normalized_shape = (normalized_shape, )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        if self.data_format == \"channels_last\":\n",
    "            return F.layer_norm(x, self.normalized_shape, self.weight, self.bias, self.eps)\n",
    "        elif self.data_format == \"channels_first\":\n",
    "            u = x.mean(1, keepdim=True)\n",
    "            s = (x - u).pow(2).mean(1, keepdim=True)\n",
    "            x = (x - u) / torch.sqrt(s + self.eps)\n",
    "            x = self.weight[:, None, None] * x + self.bias[:, None, None]\n",
    "            return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fZgyRlnnNatl"
   },
   "outputs": [],
   "source": [
    "def convnext_tiny(pretrained=False,in_22k=False, **kwargs):\n",
    "    model = ConvNeXt(depths=[3, 3, 9, 3], dims=[96, 192, 384, 768], **kwargs)\n",
    "    if pretrained:\n",
    "        url = model_urls['convnext_tiny_22k'] if in_22k else model_urls['convnext_tiny_1k']\n",
    "        checkpoint = torch.hub.load_state_dict_from_url(url=url, map_location=\"cpu\", check_hash=True)\n",
    "        model.load_state_dict(checkpoint[\"model\"])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "l5xGKEXiR1io"
   },
   "outputs": [],
   "source": [
    "model_urls = {\n",
    "    \"convnext_tiny_1k\": \"https://dl.fbaipublicfiles.com/convnext/convnext_tiny_1k_224_ema.pth\",\n",
    "    \"convnext_small_1k\": \"https://dl.fbaipublicfiles.com/convnext/convnext_small_1k_224_ema.pth\",\n",
    "    \"convnext_base_1k\": \"https://dl.fbaipublicfiles.com/convnext/convnext_base_1k_224_ema.pth\",\n",
    "    \"convnext_large_1k\": \"https://dl.fbaipublicfiles.com/convnext/convnext_large_1k_224_ema.pth\",\n",
    "    \"convnext_tiny_22k\": \"https://dl.fbaipublicfiles.com/convnext/convnext_tiny_22k_224.pth\",\n",
    "    \"convnext_small_22k\": \"https://dl.fbaipublicfiles.com/convnext/convnext_small_22k_224.pth\",\n",
    "    \"convnext_base_22k\": \"https://dl.fbaipublicfiles.com/convnext/convnext_base_22k_224.pth\",\n",
    "    \"convnext_large_22k\": \"https://dl.fbaipublicfiles.com/convnext/convnext_large_22k_224.pth\",\n",
    "    \"convnext_xlarge_22k\": \"https://dl.fbaipublicfiles.com/convnext/convnext_xlarge_22k_224.pth\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KkrcORmuMH0U"
   },
   "outputs": [],
   "source": [
    "model = convnext_tiny(pretrained= False, in_22k= False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c4I6_4UFJQrh"
   },
   "outputs": [],
   "source": [
    "pytorch_total_params = sum(p.numel() for p in model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 123,
     "status": "ok",
     "timestamp": 1648792115523,
     "user": {
      "displayName": "张月恒",
      "userId": "16697410716860166342"
     },
     "user_tz": 240
    },
    "id": "q6m6jPSrUbz4",
    "outputId": "8a9de214-16cb-456c-cee9-51d68f7dc5d6"
   },
   "outputs": [],
   "source": [
    "pytorch_total_params "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 281,
     "status": "ok",
     "timestamp": 1648829864392,
     "user": {
      "displayName": "张月恒",
      "userId": "16697410716860166342"
     },
     "user_tz": 240
    },
    "id": "GxJRVD69UdqU",
    "outputId": "2f214e92-2bb4-4ec0-f1e6-5a8c37c1ddf4"
   },
   "outputs": [],
   "source": [
    "model.head = nn.Linear(768, 200)\n",
    "# self.head = nn.Linear(dims[-1], num_classes)\n",
    "model.head.weight.data.mul_(1)\n",
    "model.head.bias.data.mul_(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UjLyD-eMRsxC"
   },
   "source": [
    "Train on the 448 1K pretrained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/"
    },
    "id": "D7pxqFIFYbJS",
    "outputId": "ac2764b6-ff33-4853-cb8b-8a046136d19e"
   },
   "outputs": [],
   "source": [
    "len_train = len(train_loader)\n",
    "\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "criterion, optimizer, scheduler, scaler = set_up(model, device, lr, weight_decay, len_train, epochs)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    train_accuracy, train_loss, learning_rate = train(model, device, batch_size, train_loader, optimizer, criterion, scheduler, scaler)\n",
    "    print(\"Epoch {}/{}: Train Acc {:.04f}%, Train Loss {:.04f}, Learning Rate {:.04f}\".format(epoch + 1, epochs, train_accuracy, train_loss, learning_rate))\n",
    "\n",
    "    if not (epoch + 1) % 10 and epoch > 0:\n",
    "        test_accuracy = evaluate(model, device, batch_size, test_loader, test_dataset)\n",
    "        print(\"Test: {:.04f}%\".format(test_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HRUvfb-kRp7z"
   },
   "source": [
    "Train on the 448 1K non-pretrained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7564905,
     "status": "ok",
     "timestamp": 1648829716581,
     "user": {
      "displayName": "张月恒",
      "userId": "16697410716860166342"
     },
     "user_tz": 240
    },
    "id": "TeDSGP_rs9_O",
    "outputId": "7cfc651b-41ce-4a00-c4b5-f2570de10cc8"
   },
   "outputs": [],
   "source": [
    "# torch.cuda.empty_cache()\n",
    "convnext_tiny = models.convnext_tiny(pretrained=False)\n",
    "convnext_tiny.classifier[2] = nn.Linear(768, 200)\n",
    "len_train = len(train_loader)\n",
    "\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "criterion, optimizer, scheduler, scaler = set_up(convnext_tiny, device, lr, weight_decay, len_train, epochs)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    train_accuracy, train_loss, learning_rate = train(convnext_tiny, device, batch_size, train_loader, optimizer, criterion, scheduler, scaler)\n",
    "    print(\"Epoch {}/{}: Train Acc {:.04f}%, Train Loss {:.04f}, Learning Rate {:.04f}\".format(epoch + 1, epochs, train_accuracy, train_loss, learning_rate))\n",
    "\n",
    "    if not (epoch + 1) % 10 and epoch > 0:\n",
    "        test_accuracy = evaluate(convnext_tiny, device, batch_size, test_loader, test_dataset)\n",
    "        print(\"Test: {:.04f}%\".format(test_accuracy))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V2tAokEERhpk"
   },
   "source": [
    "Train on the 224 1K non-pretrained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4389351,
     "status": "ok",
     "timestamp": 1648834271851,
     "user": {
      "displayName": "张月恒",
      "userId": "16697410716860166342"
     },
     "user_tz": 240
    },
    "id": "Ts6MbvBD61Fm",
    "outputId": "80e16f14-b225-4aac-f089-acba52d73b68"
   },
   "outputs": [],
   "source": [
    "len_train = len(train_loader)\n",
    "\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "criterion, optimizer, scheduler, scaler = set_up(model, device, lr, weight_decay, len_train, epochs)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    train_accuracy, train_loss, learning_rate = train(model, device, batch_size, train_loader, optimizer, criterion, scheduler, scaler)\n",
    "    print(\"Epoch {}/{}: Train Acc {:.04f}%, Train Loss {:.04f}, Learning Rate {:.04f}\".format(epoch + 1, epochs, train_accuracy, train_loss, learning_rate))\n",
    "\n",
    "    if not (epoch + 1) % 10 and epoch > 0:\n",
    "        test_accuracy = evaluate(model, device, batch_size, test_loader, test_dataset)\n",
    "        print(\"Test: {:.04f}%\".format(test_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "enZmTinr6b1g"
   },
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TY5S7SVKJbLO"
   },
   "outputs": [],
   "source": [
    "torch.save({\n",
    "                    'model_state_dict': model.state_dict(),\n",
    "                    'optimizer_state_dict': optimizer.state_dict(),\n",
    "                    'scheduler_state_dict' : scheduler.state_dict(),\n",
    "        }, \"/content/drive/MyDrive/FGVC-project\"+\"Model_\"+str(epoch))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "convenext.ipynb（副本）",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
