{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":105955,"status":"ok","timestamp":1648821893540,"user":{"displayName":"张月恒","userId":"16697410716860166342"},"user_tz":240},"id":"HgTIHpM7DVC-","outputId":"1f6b3077-13e4-4eb4-8904-9b0a6d09fb8b"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting timm\n","  Downloading timm-0.5.4-py3-none-any.whl (431 kB)\n","\u001b[?25l\r\u001b[K     |▊                               | 10 kB 32.5 MB/s eta 0:00:01\r\u001b[K     |█▌                              | 20 kB 38.4 MB/s eta 0:00:01\r\u001b[K     |██▎                             | 30 kB 36.6 MB/s eta 0:00:01\r\u001b[K     |███                             | 40 kB 16.9 MB/s eta 0:00:01\r\u001b[K     |███▉                            | 51 kB 13.9 MB/s eta 0:00:01\r\u001b[K     |████▋                           | 61 kB 16.1 MB/s eta 0:00:01\r\u001b[K     |█████▎                          | 71 kB 15.8 MB/s eta 0:00:01\r\u001b[K     |██████                          | 81 kB 15.4 MB/s eta 0:00:01\r\u001b[K     |██████▉                         | 92 kB 16.9 MB/s eta 0:00:01\r\u001b[K     |███████▋                        | 102 kB 15.1 MB/s eta 0:00:01\r\u001b[K     |████████▍                       | 112 kB 15.1 MB/s eta 0:00:01\r\u001b[K     |█████████▏                      | 122 kB 15.1 MB/s eta 0:00:01\r\u001b[K     |█████████▉                      | 133 kB 15.1 MB/s eta 0:00:01\r\u001b[K     |██████████▋                     | 143 kB 15.1 MB/s eta 0:00:01\r\u001b[K     |███████████▍                    | 153 kB 15.1 MB/s eta 0:00:01\r\u001b[K     |████████████▏                   | 163 kB 15.1 MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 174 kB 15.1 MB/s eta 0:00:01\r\u001b[K     |█████████████▊                  | 184 kB 15.1 MB/s eta 0:00:01\r\u001b[K     |██████████████▍                 | 194 kB 15.1 MB/s eta 0:00:01\r\u001b[K     |███████████████▏                | 204 kB 15.1 MB/s eta 0:00:01\r\u001b[K     |████████████████                | 215 kB 15.1 MB/s eta 0:00:01\r\u001b[K     |████████████████▊               | 225 kB 15.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████▌              | 235 kB 15.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████▎             | 245 kB 15.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 256 kB 15.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████▊            | 266 kB 15.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████▌           | 276 kB 15.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▎          | 286 kB 15.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 296 kB 15.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▉         | 307 kB 15.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▌        | 317 kB 15.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▎       | 327 kB 15.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 337 kB 15.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▉      | 348 kB 15.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▋     | 358 kB 15.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▍    | 368 kB 15.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 378 kB 15.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▉   | 389 kB 15.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▋  | 399 kB 15.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▍ | 409 kB 15.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▏| 419 kB 15.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 430 kB 15.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 431 kB 15.1 MB/s \n","\u001b[?25hRequirement already satisfied: torch>=1.4 in /usr/local/lib/python3.7/dist-packages (from timm) (1.10.0+cu111)\n","Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (from timm) (0.11.1+cu111)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.4->timm) (3.10.0.2)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchvision->timm) (1.21.5)\n","Requirement already satisfied: pillow!=8.3.0,>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision->timm) (7.1.2)\n","Installing collected packages: timm\n","Successfully installed timm-0.5.4\n","Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (0.11.1+cu111)\n","Collecting torchvision\n","  Downloading torchvision-0.12.0-cp37-cp37m-manylinux1_x86_64.whl (21.0 MB)\n","\u001b[K     |████████████████████████████████| 21.0 MB 14.8 MB/s \n","\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torchvision) (3.10.0.2)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from torchvision) (2.23.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchvision) (1.21.5)\n","Collecting torch==1.11.0\n","  Downloading torch-1.11.0-cp37-cp37m-manylinux1_x86_64.whl (750.6 MB)\n","\u001b[K     |████████████████████████████████| 750.6 MB 9.9 kB/s \n","\u001b[?25hRequirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision) (7.1.2)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision) (2.10)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision) (2021.10.8)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision) (1.24.3)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision) (3.0.4)\n","Installing collected packages: torch, torchvision\n","  Attempting uninstall: torch\n","    Found existing installation: torch 1.10.0+cu111\n","    Uninstalling torch-1.10.0+cu111:\n","      Successfully uninstalled torch-1.10.0+cu111\n","  Attempting uninstall: torchvision\n","    Found existing installation: torchvision 0.11.1+cu111\n","    Uninstalling torchvision-0.11.1+cu111:\n","      Successfully uninstalled torchvision-0.11.1+cu111\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","torchtext 0.11.0 requires torch==1.10.0, but you have torch 1.11.0 which is incompatible.\n","torchaudio 0.10.0+cu111 requires torch==1.10.0, but you have torch 1.11.0 which is incompatible.\u001b[0m\n","Successfully installed torch-1.11.0 torchvision-0.12.0\n"]}],"source":["!pip install timm\n","!pip install torchvision --upgrade\n","!mkdir StandfordDog"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Hzc7qq5tDy7Z"},"outputs":[],"source":["from functools import partial\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torchvision.models as models\n","from timm.models.layers import trunc_normal_, DropPath\n","from timm.models.registry import register_model\n","import tarfile\n","from Dataset import *\n","from prepare import *"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":21599,"status":"ok","timestamp":1648821954498,"user":{"displayName":"张月恒","userId":"16697410716860166342"},"user_tz":240},"id":"q3QVcAVfLweD","outputId":"9b7bff14-f7f7-4064-e9ef-3c983e267ee7"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WPvjgUBHFC2x"},"outputs":[],"source":["with tarfile.open('/content/drive/MyDrive/FGVC-project/CUB_200_2011.tgz', 'r:gz') as tar:\n","  tar.extractall(path='/content')\n","\n","#with tarfile.open('/content/drive/MyDrive/FGVC-project/StandfordDog/images.tar', 'r') as tar:\n","  #tar.extractall(path='/content/StandfordDog')\n","\n","#with tarfile.open('/content/drive/MyDrive/FGVC-project/StandfordDog/lists.tar', 'r') as tar:\n","  #tar.extractall(path='/content/StandfordDog')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"W-GJ7T87e2Ca"},"outputs":[],"source":["batch_size = 64\n","epochs=50\n","lr = 0.03\n","weight_decay = 1e-3\n","label_smoothing = 0.0"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4,"status":"ok","timestamp":1648829834048,"user":{"displayName":"张月恒","userId":"16697410716860166342"},"user_tz":240},"id":"Iu40C54lHccJ","outputId":"b7c78c24-8676-4e0d-efb1-4eb286a9d144"},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torchvision/transforms/transforms.py:333: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n","  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n"]}],"source":["train_transforms = transforms.Compose([transforms.Resize((256, 256), Image.BILINEAR),\n","                                       transforms.RandomCrop((224, 224)),\n","                                       transforms.RandomHorizontalFlip(),\n","                                       transforms.ToTensor(),\n","                                       transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])])\n","\n","test_transforms = transforms.Compose([transforms.Resize((256, 256), Image.BILINEAR),\n","                                        transforms.CenterCrop((224, 224)),\n","                                        transforms.ToTensor(),\n","                                        transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])])\n","\n","\n","train_dataset = CUB2011(root='/content/', transform=train_transforms, train=True, extract=False)\n","test_dataset = CUB2011(root='/content/', transform=test_transforms, train=False, extract=False)\n","\n","\n","train_loader = DataLoader(train_dataset, batch_size=batch_size, \n","                                              shuffle=True, num_workers=1)\n","test_loader = DataLoader(test_dataset, batch_size=batch_size, \n","                                          shuffle=False, num_workers=1)"]},{"cell_type":"markdown","metadata":{"id":"ydYcM2d8JSN_"},"source":["convNext base"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aLFWfT8SNZP2"},"outputs":[],"source":["class ConvNeXt(nn.Module):\n","    r\"\"\" ConvNeXt\n","        A PyTorch impl of : `A ConvNet for the 2020s`  -\n","          https://arxiv.org/pdf/2201.03545.pdf\n","    Args:\n","        in_chans (int): Number of input image channels. Default: 3\n","        num_classes (int): Number of classes for classification head. Default: 1000\n","        depths (tuple(int)): Number of blocks at each stage. Default: [3, 3, 9, 3]\n","        dims (int): Feature dimension at each stage. Default: [96, 192, 384, 768]\n","        drop_path_rate (float): Stochastic depth rate. Default: 0.\n","        layer_scale_init_value (float): Init value for Layer Scale. Default: 1e-6.\n","        head_init_scale (float): Init scaling value for classifier weights and biases. Default: 1.\n","    \"\"\"\n","    def __init__(self, in_chans=3, num_classes=1000, \n","                 depths=[3, 3, 9, 3], dims=[96, 192, 384, 768], drop_path_rate=0., \n","                 layer_scale_init_value=1e-6, head_init_scale=1.,\n","                 ):\n","        super().__init__()\n","\n","        self.downsample_layers = nn.ModuleList() # stem and 3 intermediate downsampling conv layers\n","        stem = nn.Sequential(\n","            nn.Conv2d(in_chans, dims[0], kernel_size=4, stride=4),\n","            LayerNorm(dims[0], eps=1e-6, data_format=\"channels_first\")\n","        )\n","        self.downsample_layers.append(stem)\n","        for i in range(3):\n","            downsample_layer = nn.Sequential(\n","                    LayerNorm(dims[i], eps=1e-6, data_format=\"channels_first\"),\n","                    nn.Conv2d(dims[i], dims[i+1], kernel_size=2, stride=2),\n","            )\n","            self.downsample_layers.append(downsample_layer)\n","\n","        self.stages = nn.ModuleList() # 4 feature resolution stages, each consisting of multiple residual blocks\n","        dp_rates=[x.item() for x in torch.linspace(0, drop_path_rate, sum(depths))] \n","        cur = 0\n","        for i in range(4):\n","            stage = nn.Sequential(\n","                *[Block(dim=dims[i], drop_path=dp_rates[cur + j], \n","                layer_scale_init_value=layer_scale_init_value) for j in range(depths[i])]\n","            )\n","            self.stages.append(stage)\n","            cur += depths[i]\n","\n","        self.norm = nn.LayerNorm(dims[-1], eps=1e-6) # final norm layer\n","        self.head = nn.Linear(dims[-1], num_classes)\n","\n","        self.apply(self._init_weights)\n","        self.head.weight.data.mul_(head_init_scale)\n","        self.head.bias.data.mul_(head_init_scale)\n","\n","    def _init_weights(self, m):\n","        if isinstance(m, (nn.Conv2d, nn.Linear)):\n","            trunc_normal_(m.weight, std=.02)\n","            nn.init.constant_(m.bias, 0)\n","\n","    def forward_features(self, x):\n","        for i in range(4):\n","            x = self.downsample_layers[i](x)\n","            x = self.stages[i](x)\n","        return self.norm(x.mean([-2, -1])) # global average pooling, (N, C, H, W) -> (N, C)\n","\n","    def forward(self, x):\n","        x = self.forward_features(x)\n","        x = self.head(x)\n","        return x\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VJ-Grs7ZN1x3"},"outputs":[],"source":["class Block(nn.Module):\n","    r\"\"\" ConvNeXt Block. There are two equivalent implementations:\n","    (1) DwConv -> LayerNorm (channels_first) -> 1x1 Conv -> GELU -> 1x1 Conv; all in (N, C, H, W)\n","    (2) DwConv -> Permute to (N, H, W, C); LayerNorm (channels_last) -> Linear -> GELU -> Linear; Permute back\n","    We use (2) as we find it slightly faster in PyTorch\n","    \n","    Args:\n","        dim (int): Number of input channels.\n","        drop_path (float): Stochastic depth rate. Default: 0.0\n","        layer_scale_init_value (float): Init value for Layer Scale. Default: 1e-6.\n","    \"\"\"\n","    def __init__(self, dim, drop_path=0., layer_scale_init_value=1e-6):\n","        super().__init__()\n","        self.dwconv = nn.Conv2d(dim, dim, kernel_size=7, padding=3, groups=dim) # depthwise conv\n","        self.norm = LayerNorm(dim, eps=1e-6)\n","        self.pwconv1 = nn.Linear(dim, 4 * dim) # pointwise/1x1 convs, implemented with linear layers\n","        self.act = nn.GELU()\n","        self.pwconv2 = nn.Linear(4 * dim, dim)\n","        self.gamma = nn.Parameter(layer_scale_init_value * torch.ones((dim)), \n","                                    requires_grad=True) if layer_scale_init_value > 0 else None\n","        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n","\n","    def forward(self, x):\n","        input = x\n","        x = self.dwconv(x)\n","        x = x.permute(0, 2, 3, 1) # (N, C, H, W) -> (N, H, W, C)\n","        x = self.norm(x)\n","        x = self.pwconv1(x)\n","        x = self.act(x)\n","        x = self.pwconv2(x)\n","        if self.gamma is not None:\n","            x = self.gamma * x\n","        x = x.permute(0, 3, 1, 2) # (N, H, W, C) -> (N, C, H, W)\n","\n","        x = input + self.drop_path(x)\n","        return x\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gye1M0NVNwWI"},"outputs":[],"source":["class LayerNorm(nn.Module):\n","    r\"\"\" LayerNorm that supports two data formats: channels_last (default) or channels_first. \n","    The ordering of the dimensions in the inputs. channels_last corresponds to inputs with \n","    shape (batch_size, height, width, channels) while channels_first corresponds to inputs \n","    with shape (batch_size, channels, height, width).\n","    \"\"\"\n","    def __init__(self, normalized_shape, eps=1e-6, data_format=\"channels_last\"):\n","        super().__init__()\n","        self.weight = nn.Parameter(torch.ones(normalized_shape))\n","        self.bias = nn.Parameter(torch.zeros(normalized_shape))\n","        self.eps = eps\n","        self.data_format = data_format\n","        if self.data_format not in [\"channels_last\", \"channels_first\"]:\n","            raise NotImplementedError \n","        self.normalized_shape = (normalized_shape, )\n","    \n","    def forward(self, x):\n","        if self.data_format == \"channels_last\":\n","            return F.layer_norm(x, self.normalized_shape, self.weight, self.bias, self.eps)\n","        elif self.data_format == \"channels_first\":\n","            u = x.mean(1, keepdim=True)\n","            s = (x - u).pow(2).mean(1, keepdim=True)\n","            x = (x - u) / torch.sqrt(s + self.eps)\n","            x = self.weight[:, None, None] * x + self.bias[:, None, None]\n","            return x"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fZgyRlnnNatl"},"outputs":[],"source":["def convnext_tiny(pretrained=False,in_22k=False, **kwargs):\n","    model = ConvNeXt(depths=[3, 3, 9, 3], dims=[96, 192, 384, 768], **kwargs)\n","    if pretrained:\n","        url = model_urls['convnext_tiny_22k'] if in_22k else model_urls['convnext_tiny_1k']\n","        checkpoint = torch.hub.load_state_dict_from_url(url=url, map_location=\"cpu\", check_hash=True)\n","        model.load_state_dict(checkpoint[\"model\"])\n","    return model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"l5xGKEXiR1io"},"outputs":[],"source":["model_urls = {\n","    \"convnext_tiny_1k\": \"https://dl.fbaipublicfiles.com/convnext/convnext_tiny_1k_224_ema.pth\",\n","    \"convnext_small_1k\": \"https://dl.fbaipublicfiles.com/convnext/convnext_small_1k_224_ema.pth\",\n","    \"convnext_base_1k\": \"https://dl.fbaipublicfiles.com/convnext/convnext_base_1k_224_ema.pth\",\n","    \"convnext_large_1k\": \"https://dl.fbaipublicfiles.com/convnext/convnext_large_1k_224_ema.pth\",\n","    \"convnext_tiny_22k\": \"https://dl.fbaipublicfiles.com/convnext/convnext_tiny_22k_224.pth\",\n","    \"convnext_small_22k\": \"https://dl.fbaipublicfiles.com/convnext/convnext_small_22k_224.pth\",\n","    \"convnext_base_22k\": \"https://dl.fbaipublicfiles.com/convnext/convnext_base_22k_224.pth\",\n","    \"convnext_large_22k\": \"https://dl.fbaipublicfiles.com/convnext/convnext_large_22k_224.pth\",\n","    \"convnext_xlarge_22k\": \"https://dl.fbaipublicfiles.com/convnext/convnext_xlarge_22k_224.pth\",\n","}"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KkrcORmuMH0U"},"outputs":[],"source":["model = convnext_tiny(pretrained= False, in_22k= False)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"c4I6_4UFJQrh"},"outputs":[],"source":["pytorch_total_params = sum(p.numel() for p in model.parameters())"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":123,"status":"ok","timestamp":1648792115523,"user":{"displayName":"张月恒","userId":"16697410716860166342"},"user_tz":240},"id":"q6m6jPSrUbz4","outputId":"8a9de214-16cb-456c-cee9-51d68f7dc5d6"},"outputs":[{"data":{"text/plain":["28589128"]},"execution_count":11,"metadata":{},"output_type":"execute_result"}],"source":["pytorch_total_params "]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":281,"status":"ok","timestamp":1648829864392,"user":{"displayName":"张月恒","userId":"16697410716860166342"},"user_tz":240},"id":"GxJRVD69UdqU","outputId":"2f214e92-2bb4-4ec0-f1e6-5a8c37c1ddf4"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([-2.9429e-02, -2.6101e-02,  3.3958e-02,  1.5424e-02, -1.6329e-03,\n","        -3.4336e-02, -6.3030e-03,  6.0324e-03, -2.0140e-02,  1.4907e-02,\n","        -3.1033e-02, -8.9757e-03, -2.2465e-02, -2.7120e-02,  5.9825e-03,\n","        -6.0472e-03, -1.1577e-02,  4.9838e-03,  1.9827e-02,  3.4601e-02,\n","         1.2606e-02,  6.5946e-03, -1.4502e-02, -6.8717e-03, -2.3178e-02,\n","        -1.2911e-02, -9.4146e-03,  2.3769e-02, -2.7226e-02,  1.9225e-02,\n","         1.7618e-02, -3.4812e-04,  2.2362e-02, -2.2156e-02, -3.4127e-02,\n","        -3.3828e-02, -2.8084e-02,  2.4468e-03, -3.4594e-02,  2.7760e-02,\n","        -2.4601e-02,  1.6676e-04, -1.9516e-02, -1.2142e-02,  6.7058e-03,\n","         1.5028e-02, -3.5727e-02, -3.2827e-02, -3.2057e-02, -3.4269e-02,\n","         2.3051e-02,  2.6875e-02,  1.0378e-03, -3.1191e-02,  2.9113e-02,\n","        -5.3791e-03, -3.5473e-03, -2.0198e-02, -1.6058e-02,  8.2694e-03,\n","        -1.2081e-02,  9.4416e-05,  1.4879e-02, -3.3068e-02,  2.9786e-02,\n","        -1.8863e-02,  2.3542e-02, -3.1236e-02,  1.9831e-02, -8.9216e-03,\n","         1.2455e-02, -8.0834e-03,  2.2647e-02,  2.3966e-02,  1.2448e-02,\n","        -1.6060e-02,  2.1803e-02, -1.4221e-02,  2.7484e-02,  2.0227e-02,\n","        -2.8837e-02, -1.0583e-02,  1.8660e-02,  2.7001e-02,  3.3467e-02,\n","        -3.4913e-02, -5.8882e-04, -3.0589e-03, -3.0553e-03, -3.3606e-02,\n","        -2.4465e-02, -1.8347e-02,  3.4955e-02, -4.0032e-03, -9.1484e-03,\n","         9.2463e-03,  1.7361e-02,  2.2588e-03, -3.9708e-03, -1.5669e-02,\n","        -3.1913e-02, -3.1006e-02, -2.6703e-02, -2.1715e-02,  1.9977e-03,\n","        -1.0638e-02,  2.5411e-03,  2.4246e-02, -6.6674e-03,  3.2876e-02,\n","         7.6346e-03,  2.5615e-02,  2.1134e-02, -2.5237e-02, -1.5484e-02,\n","         1.6383e-02,  3.1922e-02, -1.0710e-02,  2.5734e-02, -1.0323e-02,\n","        -3.4908e-02, -1.5021e-02, -1.7859e-02, -2.0152e-02, -2.8536e-02,\n","        -5.3349e-04, -1.2831e-02, -1.1037e-02,  3.4618e-03, -1.3197e-03,\n","        -5.9711e-04, -2.7675e-02, -3.5369e-02, -8.6284e-03, -4.2406e-04,\n","        -2.9792e-02, -1.9606e-02, -1.7779e-02, -1.6485e-02, -3.2942e-02,\n","        -2.3357e-02, -1.7701e-02,  2.3847e-02, -2.1793e-02,  2.1522e-02,\n","         2.2291e-02, -2.7953e-02,  3.2181e-02, -3.1710e-02, -9.8100e-03,\n","        -1.5297e-02,  3.5095e-02, -2.2271e-02, -3.3670e-02,  1.7883e-02,\n","        -2.2252e-02,  2.0578e-02, -2.4362e-02,  2.0839e-03, -1.0302e-02,\n","         3.3003e-03, -7.3275e-03, -1.5692e-02,  7.5817e-03, -2.0938e-02,\n","         2.0828e-02,  3.0498e-02, -3.4107e-02, -3.0922e-02, -3.5574e-02,\n","         3.4770e-02, -3.4110e-02,  9.0574e-03,  1.8776e-02, -3.3783e-02,\n","         2.3148e-02, -1.4618e-02, -3.3826e-02,  2.1132e-04, -1.9140e-02,\n","         1.4219e-02, -2.8674e-02, -2.1388e-02,  3.0405e-02, -6.1582e-03,\n","        -3.0613e-02, -3.2735e-02,  3.1185e-02,  3.0354e-02, -7.4148e-04,\n","        -2.5579e-02,  1.9783e-02, -2.0714e-02,  3.4293e-02, -1.8812e-02,\n","        -2.3005e-02, -3.1585e-02,  1.2953e-02, -2.0377e-02, -1.5821e-02])"]},"metadata":{},"execution_count":11}],"source":["model.head = nn.Linear(768, 200)\n","# self.head = nn.Linear(dims[-1], num_classes)\n","model.head.weight.data.mul_(1)\n","model.head.bias.data.mul_(1)"]},{"cell_type":"markdown","source":["Train on the 448 1K pretrained model"],"metadata":{"id":"UjLyD-eMRsxC"}},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"id":"D7pxqFIFYbJS","outputId":"ac2764b6-ff33-4853-cb8b-8a046136d19e"},"outputs":[{"name":"stderr","output_type":"stream","text":[""]},{"name":"stdout","output_type":"stream","text":["Epoch 1/50: Train Acc 0.4654%, Train Loss 5.3045, Learning Rate 0.0300\n"]},{"name":"stderr","output_type":"stream","text":[""]},{"name":"stdout","output_type":"stream","text":["Epoch 2/50: Train Acc 1.0638%, Train Loss 5.1345, Learning Rate 0.0299\n"]},{"name":"stderr","output_type":"stream","text":[""]},{"name":"stdout","output_type":"stream","text":["Epoch 3/50: Train Acc 17.4867%, Train Loss 3.7166, Learning Rate 0.0297\n"]},{"name":"stderr","output_type":"stream","text":[""]},{"name":"stdout","output_type":"stream","text":["Epoch 4/50: Train Acc 62.8158%, Train Loss 1.3654, Learning Rate 0.0295\n"]},{"name":"stderr","output_type":"stream","text":[""]},{"name":"stdout","output_type":"stream","text":["Epoch 5/50: Train Acc 81.0505%, Train Loss 0.6689, Learning Rate 0.0293\n"]},{"name":"stderr","output_type":"stream","text":[""]},{"name":"stdout","output_type":"stream","text":["Epoch 6/50: Train Acc 89.9767%, Train Loss 0.3447, Learning Rate 0.0289\n"]},{"name":"stderr","output_type":"stream","text":[""]},{"name":"stdout","output_type":"stream","text":["Epoch 7/50: Train Acc 94.3983%, Train Loss 0.1843, Learning Rate 0.0286\n"]},{"name":"stderr","output_type":"stream","text":[""]},{"name":"stdout","output_type":"stream","text":["Epoch 8/50: Train Acc 98.3876%, Train Loss 0.0662, Learning Rate 0.0281\n"]},{"name":"stderr","output_type":"stream","text":[""]},{"name":"stdout","output_type":"stream","text":["Epoch 9/50: Train Acc 99.4515%, Train Loss 0.0219, Learning Rate 0.0277\n"]},{"name":"stderr","output_type":"stream","text":[""]},{"name":"stdout","output_type":"stream","text":["Epoch 10/50: Train Acc 99.5844%, Train Loss 0.0107, Learning Rate 0.0271\n"]},{"name":"stderr","output_type":"stream","text":[""]},{"name":"stdout","output_type":"stream","text":["Test: 82.1022%\n"]},{"name":"stderr","output_type":"stream","text":[""]},{"name":"stdout","output_type":"stream","text":["Epoch 11/50: Train Acc 99.6177%, Train Loss 0.0060, Learning Rate 0.0266\n"]},{"name":"stderr","output_type":"stream","text":[""]},{"name":"stdout","output_type":"stream","text":["Epoch 12/50: Train Acc 99.6343%, Train Loss 0.0042, Learning Rate 0.0259\n"]},{"name":"stderr","output_type":"stream","text":[""]},{"name":"stdout","output_type":"stream","text":["Epoch 13/50: Train Acc 99.6343%, Train Loss 0.0035, Learning Rate 0.0253\n"]},{"name":"stderr","output_type":"stream","text":[""]},{"name":"stdout","output_type":"stream","text":["Epoch 14/50: Train Acc 99.6343%, Train Loss 0.0031, Learning Rate 0.0246\n"]},{"name":"stderr","output_type":"stream","text":[""]},{"name":"stdout","output_type":"stream","text":["Epoch 15/50: Train Acc 99.6343%, Train Loss 0.0028, Learning Rate 0.0238\n"]},{"name":"stderr","output_type":"stream","text":[""]},{"name":"stdout","output_type":"stream","text":["Epoch 16/50: Train Acc 99.6343%, Train Loss 0.0028, Learning Rate 0.0230\n"]},{"name":"stderr","output_type":"stream","text":[""]},{"name":"stdout","output_type":"stream","text":["Epoch 17/50: Train Acc 99.6343%, Train Loss 0.0023, Learning Rate 0.0222\n"]},{"name":"stderr","output_type":"stream","text":[""]},{"name":"stdout","output_type":"stream","text":["Epoch 18/50: Train Acc 99.6343%, Train Loss 0.0022, Learning Rate 0.0214\n"]},{"name":"stderr","output_type":"stream","text":[""]},{"name":"stdout","output_type":"stream","text":["Epoch 19/50: Train Acc 99.6343%, Train Loss 0.0020, Learning Rate 0.0205\n"]},{"name":"stderr","output_type":"stream","text":[""]},{"name":"stdout","output_type":"stream","text":["Epoch 20/50: Train Acc 99.6343%, Train Loss 0.0019, Learning Rate 0.0196\n"]},{"name":"stderr","output_type":"stream","text":[""]},{"name":"stdout","output_type":"stream","text":["Test: 83.1550%\n"]},{"name":"stderr","output_type":"stream","text":[""]},{"name":"stdout","output_type":"stream","text":["Epoch 21/50: Train Acc 99.6343%, Train Loss 0.0018, Learning Rate 0.0187\n"]},{"name":"stderr","output_type":"stream","text":[""]},{"name":"stdout","output_type":"stream","text":["Epoch 22/50: Train Acc 99.6343%, Train Loss 0.0017, Learning Rate 0.0178\n"]},{"name":"stderr","output_type":"stream","text":[""]},{"name":"stdout","output_type":"stream","text":["Epoch 23/50: Train Acc 99.6343%, Train Loss 0.0016, Learning Rate 0.0169\n"]},{"name":"stderr","output_type":"stream","text":[""]},{"name":"stdout","output_type":"stream","text":["Epoch 24/50: Train Acc 99.6343%, Train Loss 0.0017, Learning Rate 0.0159\n"]},{"name":"stderr","output_type":"stream","text":[""]},{"name":"stdout","output_type":"stream","text":["Epoch 25/50: Train Acc 99.6343%, Train Loss 0.0015, Learning Rate 0.0150\n"]},{"name":"stderr","output_type":"stream","text":[""]},{"name":"stdout","output_type":"stream","text":["Epoch 26/50: Train Acc 99.6343%, Train Loss 0.0015, Learning Rate 0.0141\n"]},{"name":"stderr","output_type":"stream","text":[""]},{"name":"stdout","output_type":"stream","text":["Epoch 27/50: Train Acc 99.6343%, Train Loss 0.0014, Learning Rate 0.0131\n"]},{"name":"stderr","output_type":"stream","text":[""]},{"name":"stdout","output_type":"stream","text":["Epoch 28/50: Train Acc 99.6343%, Train Loss 0.0014, Learning Rate 0.0122\n"]},{"name":"stderr","output_type":"stream","text":[""]},{"name":"stdout","output_type":"stream","text":["Epoch 29/50: Train Acc 99.6343%, Train Loss 0.0014, Learning Rate 0.0113\n"]},{"name":"stderr","output_type":"stream","text":[""]},{"name":"stdout","output_type":"stream","text":["Epoch 30/50: Train Acc 99.6343%, Train Loss 0.0013, Learning Rate 0.0104\n"]},{"name":"stderr","output_type":"stream","text":[""]},{"name":"stdout","output_type":"stream","text":["Test: 83.2413%\n"]},{"name":"stderr","output_type":"stream","text":[""]},{"name":"stdout","output_type":"stream","text":["Epoch 31/50: Train Acc 99.6343%, Train Loss 0.0013, Learning Rate 0.0095\n"]},{"name":"stderr","output_type":"stream","text":[""]},{"name":"stdout","output_type":"stream","text":["Epoch 32/50: Train Acc 99.6343%, Train Loss 0.0013, Learning Rate 0.0086\n"]},{"name":"stderr","output_type":"stream","text":[""]},{"name":"stdout","output_type":"stream","text":["Epoch 33/50: Train Acc 99.6343%, Train Loss 0.0013, Learning Rate 0.0078\n"]},{"name":"stderr","output_type":"stream","text":[""]},{"name":"stdout","output_type":"stream","text":["Epoch 34/50: Train Acc 99.6343%, Train Loss 0.0013, Learning Rate 0.0070\n"]},{"name":"stderr","output_type":"stream","text":[""]},{"name":"stdout","output_type":"stream","text":["Epoch 35/50: Train Acc 99.6343%, Train Loss 0.0012, Learning Rate 0.0062\n"]},{"name":"stderr","output_type":"stream","text":[""]},{"name":"stdout","output_type":"stream","text":["Epoch 36/50: Train Acc 99.6343%, Train Loss 0.0012, Learning Rate 0.0054\n"]},{"name":"stderr","output_type":"stream","text":[""]},{"name":"stdout","output_type":"stream","text":["Epoch 37/50: Train Acc 99.6343%, Train Loss 0.0012, Learning Rate 0.0047\n"]},{"name":"stderr","output_type":"stream","text":[""]},{"name":"stdout","output_type":"stream","text":["Epoch 38/50: Train Acc 99.6343%, Train Loss 0.0012, Learning Rate 0.0041\n"]},{"name":"stderr","output_type":"stream","text":[""]},{"name":"stdout","output_type":"stream","text":["Epoch 39/50: Train Acc 99.6343%, Train Loss 0.0012, Learning Rate 0.0034\n"]},{"name":"stderr","output_type":"stream","text":[""]},{"name":"stdout","output_type":"stream","text":["Epoch 40/50: Train Acc 99.6343%, Train Loss 0.0012, Learning Rate 0.0029\n"]},{"name":"stderr","output_type":"stream","text":[""]},{"name":"stdout","output_type":"stream","text":["Test: 83.2931%\n"]},{"name":"stderr","output_type":"stream","text":[""]},{"name":"stdout","output_type":"stream","text":["Epoch 41/50: Train Acc 99.6343%, Train Loss 0.0012, Learning Rate 0.0023\n"]},{"name":"stderr","output_type":"stream","text":[""]},{"name":"stdout","output_type":"stream","text":["Epoch 42/50: Train Acc 99.6343%, Train Loss 0.0011, Learning Rate 0.0019\n"]},{"name":"stderr","output_type":"stream","text":[""]},{"name":"stdout","output_type":"stream","text":["Epoch 43/50: Train Acc 99.6343%, Train Loss 0.0012, Learning Rate 0.0014\n"]},{"name":"stderr","output_type":"stream","text":[""]},{"name":"stdout","output_type":"stream","text":["Epoch 44/50: Train Acc 99.6343%, Train Loss 0.0012, Learning Rate 0.0011\n"]},{"name":"stderr","output_type":"stream","text":[""]},{"name":"stdout","output_type":"stream","text":["Epoch 45/50: Train Acc 99.6343%, Train Loss 0.0012, Learning Rate 0.0007\n"]},{"name":"stderr","output_type":"stream","text":[""]},{"name":"stdout","output_type":"stream","text":["Epoch 46/50: Train Acc 99.6343%, Train Loss 0.0012, Learning Rate 0.0005\n"]},{"name":"stderr","output_type":"stream","text":[""]},{"name":"stdout","output_type":"stream","text":["Epoch 47/50: Train Acc 99.6343%, Train Loss 0.0011, Learning Rate 0.0003\n"]},{"name":"stderr","output_type":"stream","text":[""]},{"name":"stdout","output_type":"stream","text":["Epoch 48/50: Train Acc 99.6343%, Train Loss 0.0012, Learning Rate 0.0001\n"]},{"name":"stderr","output_type":"stream","text":[""]},{"name":"stdout","output_type":"stream","text":["Epoch 49/50: Train Acc 99.6343%, Train Loss 0.0011, Learning Rate 0.0000\n"]},{"name":"stderr","output_type":"stream","text":[""]},{"name":"stdout","output_type":"stream","text":["Epoch 50/50: Train Acc 99.6343%, Train Loss 0.0012, Learning Rate 0.0000\n"]},{"name":"stderr","output_type":"stream","text":["                                                                     "]},{"name":"stdout","output_type":"stream","text":["Test: 83.3103%\n"]},{"name":"stderr","output_type":"stream","text":["\r"]}],"source":["len_train = len(train_loader)\n","\n","\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","\n","criterion, optimizer, scheduler, scaler = set_up(model, device, lr, weight_decay, len_train, epochs)\n","\n","for epoch in range(epochs):\n","    train_accuracy, train_loss, learning_rate = train(model, device, batch_size, train_loader, optimizer, criterion, scheduler, scaler)\n","    print(\"Epoch {}/{}: Train Acc {:.04f}%, Train Loss {:.04f}, Learning Rate {:.04f}\".format(epoch + 1, epochs, train_accuracy, train_loss, learning_rate))\n","\n","    if not (epoch + 1) % 10 and epoch > 0:\n","        test_accuracy = evaluate(model, device, batch_size, test_loader, test_dataset)\n","        print(\"Test: {:.04f}%\".format(test_accuracy))"]},{"cell_type":"markdown","source":["Train on the 448 1K non-pretrained model"],"metadata":{"id":"HRUvfb-kRp7z"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"TeDSGP_rs9_O","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1648829716581,"user_tz":240,"elapsed":7564905,"user":{"displayName":"张月恒","userId":"16697410716860166342"}},"outputId":"7cfc651b-41ce-4a00-c4b5-f2570de10cc8"},"outputs":[{"output_type":"stream","name":"stderr","text":["Train:   0%|          | 0/188 [00:01<?, ?it/s, acc=3.1250%, loss=5.3674, lr=0.0300, num_correct=1]/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n","  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1/50: Train Acc 0.7480%, Train Loss 5.5125, Learning Rate 0.0300\n"]},{"output_type":"stream","name":"stderr","text":[""]},{"output_type":"stream","name":"stdout","text":["Epoch 2/50: Train Acc 0.6649%, Train Loss 5.2306, Learning Rate 0.0299\n"]},{"output_type":"stream","name":"stderr","text":[""]},{"output_type":"stream","name":"stdout","text":["Epoch 3/50: Train Acc 1.1636%, Train Loss 5.1808, Learning Rate 0.0297\n"]},{"output_type":"stream","name":"stderr","text":[""]},{"output_type":"stream","name":"stdout","text":["Epoch 4/50: Train Acc 0.9641%, Train Loss 5.1477, Learning Rate 0.0295\n"]},{"output_type":"stream","name":"stderr","text":[""]},{"output_type":"stream","name":"stdout","text":["Epoch 5/50: Train Acc 1.3464%, Train Loss 5.1366, Learning Rate 0.0293\n"]},{"output_type":"stream","name":"stderr","text":[""]},{"output_type":"stream","name":"stdout","text":["Epoch 6/50: Train Acc 1.2467%, Train Loss 5.1176, Learning Rate 0.0289\n"]},{"output_type":"stream","name":"stderr","text":[""]},{"output_type":"stream","name":"stdout","text":["Epoch 7/50: Train Acc 0.9309%, Train Loss 5.1063, Learning Rate 0.0286\n"]},{"output_type":"stream","name":"stderr","text":[""]},{"output_type":"stream","name":"stdout","text":["Epoch 8/50: Train Acc 1.2799%, Train Loss 5.0965, Learning Rate 0.0281\n"]},{"output_type":"stream","name":"stderr","text":[""]},{"output_type":"stream","name":"stdout","text":["Epoch 9/50: Train Acc 1.4461%, Train Loss 5.0909, Learning Rate 0.0277\n"]},{"output_type":"stream","name":"stderr","text":[""]},{"output_type":"stream","name":"stdout","text":["Epoch 10/50: Train Acc 1.4129%, Train Loss 5.0791, Learning Rate 0.0271\n"]},{"output_type":"stream","name":"stderr","text":[""]},{"output_type":"stream","name":"stdout","text":["Test: 1.3980%\n"]},{"output_type":"stream","name":"stderr","text":[""]},{"output_type":"stream","name":"stdout","text":["Epoch 11/50: Train Acc 1.4794%, Train Loss 5.0554, Learning Rate 0.0266\n"]},{"output_type":"stream","name":"stderr","text":[""]},{"output_type":"stream","name":"stdout","text":["Epoch 12/50: Train Acc 1.3963%, Train Loss 5.0314, Learning Rate 0.0259\n"]},{"output_type":"stream","name":"stderr","text":[""]},{"output_type":"stream","name":"stdout","text":["Epoch 13/50: Train Acc 1.7453%, Train Loss 5.0241, Learning Rate 0.0253\n"]},{"output_type":"stream","name":"stderr","text":[""]},{"output_type":"stream","name":"stdout","text":["Epoch 14/50: Train Acc 1.7786%, Train Loss 5.0145, Learning Rate 0.0246\n"]},{"output_type":"stream","name":"stderr","text":[""]},{"output_type":"stream","name":"stdout","text":["Epoch 15/50: Train Acc 1.7620%, Train Loss 5.0074, Learning Rate 0.0238\n"]},{"output_type":"stream","name":"stderr","text":[""]},{"output_type":"stream","name":"stdout","text":["Epoch 16/50: Train Acc 1.6789%, Train Loss 5.0065, Learning Rate 0.0230\n"]},{"output_type":"stream","name":"stderr","text":[""]},{"output_type":"stream","name":"stdout","text":["Epoch 17/50: Train Acc 2.0778%, Train Loss 4.9972, Learning Rate 0.0222\n"]},{"output_type":"stream","name":"stderr","text":[""]},{"output_type":"stream","name":"stdout","text":["Epoch 18/50: Train Acc 1.8285%, Train Loss 4.9917, Learning Rate 0.0214\n"]},{"output_type":"stream","name":"stderr","text":[""]},{"output_type":"stream","name":"stdout","text":["Epoch 19/50: Train Acc 1.8285%, Train Loss 4.9922, Learning Rate 0.0205\n"]},{"output_type":"stream","name":"stderr","text":[""]},{"output_type":"stream","name":"stdout","text":["Epoch 20/50: Train Acc 1.6622%, Train Loss 4.9868, Learning Rate 0.0196\n"]},{"output_type":"stream","name":"stderr","text":[""]},{"output_type":"stream","name":"stdout","text":["Test: 1.7432%\n"]},{"output_type":"stream","name":"stderr","text":[""]},{"output_type":"stream","name":"stdout","text":["Epoch 21/50: Train Acc 1.9116%, Train Loss 4.9822, Learning Rate 0.0187\n"]},{"output_type":"stream","name":"stderr","text":[""]},{"output_type":"stream","name":"stdout","text":["Epoch 22/50: Train Acc 1.9282%, Train Loss 4.9762, Learning Rate 0.0178\n"]},{"output_type":"stream","name":"stderr","text":[""]},{"output_type":"stream","name":"stdout","text":["Epoch 23/50: Train Acc 1.8617%, Train Loss 4.9719, Learning Rate 0.0169\n"]},{"output_type":"stream","name":"stderr","text":[""]},{"output_type":"stream","name":"stdout","text":["Epoch 24/50: Train Acc 2.0612%, Train Loss 4.9733, Learning Rate 0.0159\n"]},{"output_type":"stream","name":"stderr","text":[""]},{"output_type":"stream","name":"stdout","text":["Epoch 25/50: Train Acc 2.2108%, Train Loss 4.9704, Learning Rate 0.0150\n"]},{"output_type":"stream","name":"stderr","text":[""]},{"output_type":"stream","name":"stdout","text":["Epoch 26/50: Train Acc 2.0279%, Train Loss 4.9626, Learning Rate 0.0141\n"]},{"output_type":"stream","name":"stderr","text":[""]},{"output_type":"stream","name":"stdout","text":["Epoch 27/50: Train Acc 1.9282%, Train Loss 4.9652, Learning Rate 0.0131\n"]},{"output_type":"stream","name":"stderr","text":[""]},{"output_type":"stream","name":"stdout","text":["Epoch 28/50: Train Acc 2.2606%, Train Loss 4.9605, Learning Rate 0.0122\n"]},{"output_type":"stream","name":"stderr","text":[""]},{"output_type":"stream","name":"stdout","text":["Epoch 29/50: Train Acc 2.1443%, Train Loss 4.9573, Learning Rate 0.0113\n"]},{"output_type":"stream","name":"stderr","text":[""]},{"output_type":"stream","name":"stdout","text":["Epoch 30/50: Train Acc 2.3936%, Train Loss 4.9568, Learning Rate 0.0104\n"]},{"output_type":"stream","name":"stderr","text":[""]},{"output_type":"stream","name":"stdout","text":["Test: 1.7604%\n"]},{"output_type":"stream","name":"stderr","text":[""]},{"output_type":"stream","name":"stdout","text":["Epoch 31/50: Train Acc 2.0944%, Train Loss 4.9517, Learning Rate 0.0095\n"]},{"output_type":"stream","name":"stderr","text":[""]},{"output_type":"stream","name":"stdout","text":["Epoch 32/50: Train Acc 2.1609%, Train Loss 4.9490, Learning Rate 0.0086\n"]},{"output_type":"stream","name":"stderr","text":[""]},{"output_type":"stream","name":"stdout","text":["Epoch 33/50: Train Acc 2.0279%, Train Loss 4.9476, Learning Rate 0.0078\n"]},{"output_type":"stream","name":"stderr","text":[""]},{"output_type":"stream","name":"stdout","text":["Epoch 34/50: Train Acc 2.2773%, Train Loss 4.9452, Learning Rate 0.0070\n"]},{"output_type":"stream","name":"stderr","text":[""]},{"output_type":"stream","name":"stdout","text":["Epoch 35/50: Train Acc 2.3438%, Train Loss 4.9417, Learning Rate 0.0062\n"]},{"output_type":"stream","name":"stderr","text":[""]},{"output_type":"stream","name":"stdout","text":["Epoch 36/50: Train Acc 2.4102%, Train Loss 4.9395, Learning Rate 0.0054\n"]},{"output_type":"stream","name":"stderr","text":[""]},{"output_type":"stream","name":"stdout","text":["Epoch 37/50: Train Acc 2.4767%, Train Loss 4.9355, Learning Rate 0.0047\n"]},{"output_type":"stream","name":"stderr","text":[""]},{"output_type":"stream","name":"stdout","text":["Epoch 38/50: Train Acc 2.2108%, Train Loss 4.9333, Learning Rate 0.0041\n"]},{"output_type":"stream","name":"stderr","text":[""]},{"output_type":"stream","name":"stdout","text":["Epoch 39/50: Train Acc 2.4934%, Train Loss 4.9317, Learning Rate 0.0034\n"]},{"output_type":"stream","name":"stderr","text":[""]},{"output_type":"stream","name":"stdout","text":["Epoch 40/50: Train Acc 2.6430%, Train Loss 4.9293, Learning Rate 0.0029\n"]},{"output_type":"stream","name":"stderr","text":[""]},{"output_type":"stream","name":"stdout","text":["Test: 1.8295%\n"]},{"output_type":"stream","name":"stderr","text":[""]},{"output_type":"stream","name":"stdout","text":["Epoch 41/50: Train Acc 2.5266%, Train Loss 4.9271, Learning Rate 0.0023\n"]},{"output_type":"stream","name":"stderr","text":[""]},{"output_type":"stream","name":"stdout","text":["Epoch 42/50: Train Acc 2.4934%, Train Loss 4.9261, Learning Rate 0.0019\n"]},{"output_type":"stream","name":"stderr","text":[""]},{"output_type":"stream","name":"stdout","text":["Epoch 43/50: Train Acc 2.6263%, Train Loss 4.9243, Learning Rate 0.0014\n"]},{"output_type":"stream","name":"stderr","text":[""]},{"output_type":"stream","name":"stdout","text":["Epoch 44/50: Train Acc 2.7094%, Train Loss 4.9199, Learning Rate 0.0011\n"]},{"output_type":"stream","name":"stderr","text":[""]},{"output_type":"stream","name":"stdout","text":["Epoch 45/50: Train Acc 2.5765%, Train Loss 4.9201, Learning Rate 0.0007\n"]},{"output_type":"stream","name":"stderr","text":[""]},{"output_type":"stream","name":"stdout","text":["Epoch 46/50: Train Acc 2.6762%, Train Loss 4.9178, Learning Rate 0.0005\n"]},{"output_type":"stream","name":"stderr","text":[""]},{"output_type":"stream","name":"stdout","text":["Epoch 47/50: Train Acc 2.6430%, Train Loss 4.9180, Learning Rate 0.0003\n"]},{"output_type":"stream","name":"stderr","text":[""]},{"output_type":"stream","name":"stdout","text":["Epoch 48/50: Train Acc 2.6596%, Train Loss 4.9168, Learning Rate 0.0001\n"]},{"output_type":"stream","name":"stderr","text":[""]},{"output_type":"stream","name":"stdout","text":["Epoch 49/50: Train Acc 2.7593%, Train Loss 4.9161, Learning Rate 0.0000\n"]},{"output_type":"stream","name":"stderr","text":[""]},{"output_type":"stream","name":"stdout","text":["Epoch 50/50: Train Acc 2.8092%, Train Loss 4.9159, Learning Rate 0.0000\n"]},{"output_type":"stream","name":"stderr","text":["                                                                    "]},{"output_type":"stream","name":"stdout","text":["Test: 1.9676%\n"]},{"output_type":"stream","name":"stderr","text":["\r"]}],"source":["# torch.cuda.empty_cache()\n","convnext_tiny = models.convnext_tiny(pretrained=False)\n","convnext_tiny.classifier[2] = nn.Linear(768, 200)\n","len_train = len(train_loader)\n","\n","\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","\n","criterion, optimizer, scheduler, scaler = set_up(convnext_tiny, device, lr, weight_decay, len_train, epochs)\n","\n","for epoch in range(epochs):\n","    train_accuracy, train_loss, learning_rate = train(convnext_tiny, device, batch_size, train_loader, optimizer, criterion, scheduler, scaler)\n","    print(\"Epoch {}/{}: Train Acc {:.04f}%, Train Loss {:.04f}, Learning Rate {:.04f}\".format(epoch + 1, epochs, train_accuracy, train_loss, learning_rate))\n","\n","    if not (epoch + 1) % 10 and epoch > 0:\n","        test_accuracy = evaluate(convnext_tiny, device, batch_size, test_loader, test_dataset)\n","        print(\"Test: {:.04f}%\".format(test_accuracy))\n"]},{"cell_type":"markdown","source":["Train on the 224 1K non-pretrained model"],"metadata":{"id":"V2tAokEERhpk"}},{"cell_type":"code","source":["len_train = len(train_loader)\n","\n","\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","\n","criterion, optimizer, scheduler, scaler = set_up(model, device, lr, weight_decay, len_train, epochs)\n","\n","for epoch in range(epochs):\n","    train_accuracy, train_loss, learning_rate = train(model, device, batch_size, train_loader, optimizer, criterion, scheduler, scaler)\n","    print(\"Epoch {}/{}: Train Acc {:.04f}%, Train Loss {:.04f}, Learning Rate {:.04f}\".format(epoch + 1, epochs, train_accuracy, train_loss, learning_rate))\n","\n","    if not (epoch + 1) % 10 and epoch > 0:\n","        test_accuracy = evaluate(model, device, batch_size, test_loader, test_dataset)\n","        print(\"Test: {:.04f}%\".format(test_accuracy))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Ts6MbvBD61Fm","executionInfo":{"status":"ok","timestamp":1648834271851,"user_tz":240,"elapsed":4389351,"user":{"displayName":"张月恒","userId":"16697410716860166342"}},"outputId":"80e16f14-b225-4aac-f089-acba52d73b68"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["Train:   0%|          | 0/94 [00:00<?, ?it/s, acc=0.0000%, loss=5.4352, lr=0.0300, num_correct=0]/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n","  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1/50: Train Acc 0.7314%, Train Loss 5.9827, Learning Rate 0.0300\n"]},{"output_type":"stream","name":"stderr","text":[""]},{"output_type":"stream","name":"stdout","text":["Epoch 2/50: Train Acc 1.0472%, Train Loss 5.2818, Learning Rate 0.0299\n"]},{"output_type":"stream","name":"stderr","text":[""]},{"output_type":"stream","name":"stdout","text":["Epoch 3/50: Train Acc 1.3132%, Train Loss 5.1345, Learning Rate 0.0297\n"]},{"output_type":"stream","name":"stderr","text":[""]},{"output_type":"stream","name":"stdout","text":["Epoch 4/50: Train Acc 1.4129%, Train Loss 5.0924, Learning Rate 0.0295\n"]},{"output_type":"stream","name":"stderr","text":[""]},{"output_type":"stream","name":"stdout","text":["Epoch 5/50: Train Acc 1.4129%, Train Loss 5.0783, Learning Rate 0.0293\n"]},{"output_type":"stream","name":"stderr","text":[""]},{"output_type":"stream","name":"stdout","text":["Epoch 6/50: Train Acc 1.8285%, Train Loss 5.0683, Learning Rate 0.0289\n"]},{"output_type":"stream","name":"stderr","text":[""]},{"output_type":"stream","name":"stdout","text":["Epoch 7/50: Train Acc 1.8451%, Train Loss 5.0497, Learning Rate 0.0286\n"]},{"output_type":"stream","name":"stderr","text":[""]},{"output_type":"stream","name":"stdout","text":["Epoch 8/50: Train Acc 1.7287%, Train Loss 5.0401, Learning Rate 0.0281\n"]},{"output_type":"stream","name":"stderr","text":[""]},{"output_type":"stream","name":"stdout","text":["Epoch 9/50: Train Acc 1.8617%, Train Loss 5.0423, Learning Rate 0.0277\n"]},{"output_type":"stream","name":"stderr","text":[""]},{"output_type":"stream","name":"stdout","text":["Epoch 10/50: Train Acc 1.6622%, Train Loss 5.0329, Learning Rate 0.0271\n"]},{"output_type":"stream","name":"stderr","text":[""]},{"output_type":"stream","name":"stdout","text":["Test: 1.5533%\n"]},{"output_type":"stream","name":"stderr","text":[""]},{"output_type":"stream","name":"stdout","text":["Epoch 11/50: Train Acc 1.7453%, Train Loss 5.0265, Learning Rate 0.0266\n"]},{"output_type":"stream","name":"stderr","text":[""]},{"output_type":"stream","name":"stdout","text":["Epoch 12/50: Train Acc 1.7121%, Train Loss 5.0208, Learning Rate 0.0259\n"]},{"output_type":"stream","name":"stderr","text":[""]},{"output_type":"stream","name":"stdout","text":["Epoch 13/50: Train Acc 2.0612%, Train Loss 5.0203, Learning Rate 0.0253\n"]},{"output_type":"stream","name":"stderr","text":[""]},{"output_type":"stream","name":"stdout","text":["Epoch 14/50: Train Acc 1.8451%, Train Loss 5.0147, Learning Rate 0.0246\n"]},{"output_type":"stream","name":"stderr","text":[""]},{"output_type":"stream","name":"stdout","text":["Epoch 15/50: Train Acc 1.8783%, Train Loss 5.0091, Learning Rate 0.0238\n"]},{"output_type":"stream","name":"stderr","text":[""]},{"output_type":"stream","name":"stdout","text":["Epoch 16/50: Train Acc 1.8617%, Train Loss 5.0035, Learning Rate 0.0230\n"]},{"output_type":"stream","name":"stderr","text":[""]},{"output_type":"stream","name":"stdout","text":["Epoch 17/50: Train Acc 1.9282%, Train Loss 5.0048, Learning Rate 0.0222\n"]},{"output_type":"stream","name":"stderr","text":[""]},{"output_type":"stream","name":"stdout","text":["Epoch 18/50: Train Acc 2.0445%, Train Loss 4.9982, Learning Rate 0.0214\n"]},{"output_type":"stream","name":"stderr","text":[""]},{"output_type":"stream","name":"stdout","text":["Epoch 19/50: Train Acc 2.0445%, Train Loss 4.9960, Learning Rate 0.0205\n"]},{"output_type":"stream","name":"stderr","text":[""]},{"output_type":"stream","name":"stdout","text":["Epoch 20/50: Train Acc 2.0778%, Train Loss 4.9927, Learning Rate 0.0196\n"]},{"output_type":"stream","name":"stderr","text":[""]},{"output_type":"stream","name":"stdout","text":["Test: 1.8985%\n"]},{"output_type":"stream","name":"stderr","text":[""]},{"output_type":"stream","name":"stdout","text":["Epoch 21/50: Train Acc 1.8285%, Train Loss 4.9886, Learning Rate 0.0187\n"]},{"output_type":"stream","name":"stderr","text":[""]},{"output_type":"stream","name":"stdout","text":["Epoch 22/50: Train Acc 1.8949%, Train Loss 4.9885, Learning Rate 0.0178\n"]},{"output_type":"stream","name":"stderr","text":[""]},{"output_type":"stream","name":"stdout","text":["Epoch 23/50: Train Acc 1.9947%, Train Loss 4.9823, Learning Rate 0.0169\n"]},{"output_type":"stream","name":"stderr","text":[""]},{"output_type":"stream","name":"stdout","text":["Epoch 24/50: Train Acc 1.7287%, Train Loss 4.9815, Learning Rate 0.0159\n"]},{"output_type":"stream","name":"stderr","text":[""]},{"output_type":"stream","name":"stdout","text":["Epoch 25/50: Train Acc 2.0612%, Train Loss 4.9738, Learning Rate 0.0150\n"]},{"output_type":"stream","name":"stderr","text":[""]},{"output_type":"stream","name":"stdout","text":["Epoch 26/50: Train Acc 2.2440%, Train Loss 4.9755, Learning Rate 0.0141\n"]},{"output_type":"stream","name":"stderr","text":[""]},{"output_type":"stream","name":"stdout","text":["Epoch 27/50: Train Acc 1.9448%, Train Loss 4.9684, Learning Rate 0.0131\n"]},{"output_type":"stream","name":"stderr","text":[""]},{"output_type":"stream","name":"stdout","text":["Epoch 28/50: Train Acc 2.3438%, Train Loss 4.9696, Learning Rate 0.0122\n"]},{"output_type":"stream","name":"stderr","text":[""]},{"output_type":"stream","name":"stdout","text":["Epoch 29/50: Train Acc 2.1277%, Train Loss 4.9656, Learning Rate 0.0113\n"]},{"output_type":"stream","name":"stderr","text":[""]},{"output_type":"stream","name":"stdout","text":["Epoch 30/50: Train Acc 2.0279%, Train Loss 4.9624, Learning Rate 0.0104\n"]},{"output_type":"stream","name":"stderr","text":[""]},{"output_type":"stream","name":"stdout","text":["Test: 1.8640%\n"]},{"output_type":"stream","name":"stderr","text":[""]},{"output_type":"stream","name":"stdout","text":["Epoch 31/50: Train Acc 2.4269%, Train Loss 4.9620, Learning Rate 0.0095\n"]},{"output_type":"stream","name":"stderr","text":[""]},{"output_type":"stream","name":"stdout","text":["Epoch 32/50: Train Acc 2.4102%, Train Loss 4.9566, Learning Rate 0.0086\n"]},{"output_type":"stream","name":"stderr","text":[""]},{"output_type":"stream","name":"stdout","text":["Epoch 33/50: Train Acc 2.3438%, Train Loss 4.9550, Learning Rate 0.0078\n"]},{"output_type":"stream","name":"stderr","text":[""]},{"output_type":"stream","name":"stdout","text":["Epoch 34/50: Train Acc 2.4102%, Train Loss 4.9527, Learning Rate 0.0070\n"]},{"output_type":"stream","name":"stderr","text":[""]},{"output_type":"stream","name":"stdout","text":["Epoch 35/50: Train Acc 2.1277%, Train Loss 4.9511, Learning Rate 0.0062\n"]},{"output_type":"stream","name":"stderr","text":[""]},{"output_type":"stream","name":"stdout","text":["Epoch 36/50: Train Acc 2.3936%, Train Loss 4.9464, Learning Rate 0.0054\n"]},{"output_type":"stream","name":"stderr","text":[""]},{"output_type":"stream","name":"stdout","text":["Epoch 37/50: Train Acc 2.6263%, Train Loss 4.9436, Learning Rate 0.0047\n"]},{"output_type":"stream","name":"stderr","text":[""]},{"output_type":"stream","name":"stdout","text":["Epoch 38/50: Train Acc 2.4102%, Train Loss 4.9424, Learning Rate 0.0041\n"]},{"output_type":"stream","name":"stderr","text":[""]},{"output_type":"stream","name":"stdout","text":["Epoch 39/50: Train Acc 2.4934%, Train Loss 4.9388, Learning Rate 0.0034\n"]},{"output_type":"stream","name":"stderr","text":[""]},{"output_type":"stream","name":"stdout","text":["Epoch 40/50: Train Acc 2.6762%, Train Loss 4.9383, Learning Rate 0.0029\n"]},{"output_type":"stream","name":"stderr","text":[""]},{"output_type":"stream","name":"stdout","text":["Test: 1.8985%\n"]},{"output_type":"stream","name":"stderr","text":[""]},{"output_type":"stream","name":"stdout","text":["Epoch 41/50: Train Acc 2.6097%, Train Loss 4.9349, Learning Rate 0.0023\n"]},{"output_type":"stream","name":"stderr","text":[""]},{"output_type":"stream","name":"stdout","text":["Epoch 42/50: Train Acc 2.7593%, Train Loss 4.9324, Learning Rate 0.0019\n"]},{"output_type":"stream","name":"stderr","text":[""]},{"output_type":"stream","name":"stdout","text":["Epoch 43/50: Train Acc 2.9422%, Train Loss 4.9292, Learning Rate 0.0014\n"]},{"output_type":"stream","name":"stderr","text":[""]},{"output_type":"stream","name":"stdout","text":["Epoch 44/50: Train Acc 2.8424%, Train Loss 4.9294, Learning Rate 0.0011\n"]},{"output_type":"stream","name":"stderr","text":[""]},{"output_type":"stream","name":"stdout","text":["Epoch 45/50: Train Acc 2.7094%, Train Loss 4.9279, Learning Rate 0.0007\n"]},{"output_type":"stream","name":"stderr","text":[""]},{"output_type":"stream","name":"stdout","text":["Epoch 46/50: Train Acc 2.7261%, Train Loss 4.9246, Learning Rate 0.0005\n"]},{"output_type":"stream","name":"stderr","text":[""]},{"output_type":"stream","name":"stdout","text":["Epoch 47/50: Train Acc 2.8923%, Train Loss 4.9230, Learning Rate 0.0003\n"]},{"output_type":"stream","name":"stderr","text":[""]},{"output_type":"stream","name":"stdout","text":["Epoch 48/50: Train Acc 2.6928%, Train Loss 4.9245, Learning Rate 0.0001\n"]},{"output_type":"stream","name":"stderr","text":[""]},{"output_type":"stream","name":"stdout","text":["Epoch 49/50: Train Acc 2.6928%, Train Loss 4.9242, Learning Rate 0.0000\n"]},{"output_type":"stream","name":"stderr","text":[""]},{"output_type":"stream","name":"stdout","text":["Epoch 50/50: Train Acc 2.7759%, Train Loss 4.9229, Learning Rate 0.0000\n"]},{"output_type":"stream","name":"stderr","text":["                                                                  "]},{"output_type":"stream","name":"stdout","text":["Test: 1.7777%\n"]},{"output_type":"stream","name":"stderr","text":["\r"]}]},{"cell_type":"code","source":["torch.cuda.empty_cache()"],"metadata":{"id":"enZmTinr6b1g"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TY5S7SVKJbLO"},"outputs":[],"source":["torch.save({\n","                    'model_state_dict': model.state_dict(),\n","                    'optimizer_state_dict': optimizer.state_dict(),\n","                    'scheduler_state_dict' : scheduler.state_dict(),\n","        }, \"/content/drive/MyDrive/FGVC-project\"+\"Model_\"+str(epoch))"]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"machine_shape":"hm","name":"convenext.ipynb（副本）","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}